# 书评抓取

[scrapy爬取豆瓣电影 - 简书](https://www.jianshu.com/p/ecd41f6986cc)

跑爬虫的命令：

```
scrapy crawl review -o data.json
```

## 00. 新建项目

```
scrapy startproject blog
scrapy genspider basic web
```

## 01. 设置问题

### 1. 有关身份验证

正常抓取是失败的，必须要在 settings.py 里添加自己的客户端信息。

```
USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.100 Safari/537.36'
```

然后抓取验证下：scrapy shell 'https://book.douban.com/subject/26708119/reviews'

### 2. 输出的编码格式

```
FEED_EXPORT_ENCODING = 'utf-8'
```

## 02. xpath 表达式试验抓取内容

```
response.xpath('//div[@class="main review-item"]')
response.xpath('//div[@class="main-bd"]')
response.xpath('//div[@class="main-bd"]//text()').extract() 
response.xpath('//div[@class="review-short"]
response.xpath('//div[@class="review-short"]//text()').extract()
response.xpath('//div[@class="short-content"]')
response.xpath('//div[@class="short-content"]//text()').extract() 
```

这是没「展开」的短评。

发现展开这个动作需要 JS 参与，是动态的。开发者工具里把 JS 禁止掉后，页面里点「展开」是没用的。在没有动态抓取的知识之前，发现只有下面的 xpath 表达式有效。

```
response.xpath('//*[contains(@class,"clearfix")]//text()').extract()
```

但跑到 scrapy 里抓取，发现抓取的结果还是没有「展开」的信息。

## 03. 问题汇总

1、验证的问题抓取不了信息。

解决办法：settings.py 里添加自己的客户端信息。

2、输出的 json 内容是乱码。

解决办法：settings.py 里添加输出的编码格式。

3、修改源码后抓取的结果一直不变。

解决办法：把旧版「review.py」删掉。scrapy 框架里只能有一个爬虫文件，在另存新文件里修稿源码的话，运行还是按旧版的运行的。

## 04. 代码

### 1. 翻页抓单个元素

```py
# -*- coding: utf-8 -*-
import scrapy
from readreview.items import ReadreviewItem

class ReviewSpider(scrapy.Spider):
    name = 'review'
    # allowed_domains = ['web']
    start_urls = ['https://book.douban.com/subject/26708119/reviews']

    def parse(self, response):
        item = ReadreviewItem()

        item['readview'] = response.xpath('//div[@class="short-content"]//text()').extract() 
        
        yield item

        # the next page
        urls = response.xpath('//span[@class="next"]/a')
        for url in urls:
            yield response.follow(url, callback=self.parse)
```

### 2. 成品

### 01 版本

```py
# -*- coding: utf-8 -*-
import scrapy
from readreview.items import ReadreviewItem
import re

class ReviewSpider(scrapy.Spider):
    name = 'review'
    # allowed_domains = ['web']
    start_urls = ['https://book.douban.com/subject/26278639/reviews']

    def parse(self, response):
        urls = response.xpath('//div[@class="main-bd"]/h2/a')

        for url in urls:
            yield response.follow(url, callback=self.parse_review_pages)

        # the next page
        # the reason for using lists is a puzzle
        next_url = response.xpath('//span[@class="next"]/a')
        for url in next_url:
            yield response.follow(url, callback=self.parse)
        # next_page = response.xpath('//span[@class="next"]/a')
        # if next_page:
        #     yield response.follow(next_page, callback=self.parse)

    def parse_review_pages(self, response):
        item = ReadreviewItem()

        item['title'] = response.xpath('//div[@class="article"]/h1/span/text()').extract()
        item['author'] = response.xpath('//header[@class="main-hd"]/a/span/text()').extract()
        item['date'] = response.xpath('//span[@class="main-meta"]/text()').extract()
        item['url'] = ['['+response.xpath('//div[@class="article"]/h1/span/text()').extract()[0]+']('+response.xpath('//meta[@property="og:url"]/@content').extract()[0]+')']
        item['collect'] = int(re.findall(r"\d+\.?\d*",response.xpath('//button[contains(@class,"useful")]/text()').extract()[0])[0])
        item['readview'] = response.xpath('//div[@class="review-content clearfix"]//text()').extract()
        
        yield item

```

```py
# -*- coding:utf-8 -*-

import glob, os
import json

import modify as md

# 获得桌上所有 md 文件名并对其进行分割，然后在 for 循环里进行处理
for infile in glob.glob("/Users/Daglas/Desktop/*.json"):
    filename, ext = os.path.splitext(infile)

    # 读取文件，文件名「filename + ".md"」是关键
    with open(filename + ".json") as file_obj:
        dictls = json.load(fp=file_obj)
        # print(dictls[:8])

    views = []
    for reviews in sorted(dictls, key = lambda e:e.__getitem__('collect'), reverse=True):
        views.append(reviews['title'])
        views.append(reviews['author'])
        views.append(reviews['date'])
        views.append(reviews['url'])
        # views.append(reviews['collect'])
        views.append(reviews['readview'])
    
    # print(views[:4])

    # 对文字处理并写入文件
    with open(filename + ".md", 'w') as file_obj:
        for view in views:
            for line in view:
                if line != '\n':
                    # line = line.replace('\n', '')
                    line = md.modify_text(line)
                    file_obj.write(line + "\n\n")

```

### 02 版

```py
# -*- coding: utf-8 -*-
import scrapy
from readreview.items import ReadreviewItem
import re

class ReviewSpider(scrapy.Spider):
    name = 'review'
    # allowed_domains = ['web']
    start_urls = ['https://book.douban.com/subject/25752043/reviews']

    def parse(self, response):
        urls = response.xpath('//div[@class="main-bd"]/h2/a')

        for url in urls:
            yield response.follow(url, callback=self.parse_review_pages)

        # the next page
        # the reason for using lists is a puzzle
        next_url = response.xpath('//span[@class="next"]/a')
        for url in next_url:
            yield response.follow(url, callback=self.parse)
        # next_page = response.xpath('//span[@class="next"]/a')
        # if next_page:
        #     yield response.follow(next_page, callback=self.parse)

    def parse_review_pages(self, response):
        item = ReadreviewItem()

        item['title'] = response.xpath('//div[@class="article"]/h1/span/text()').extract()[0]
        item['author'] = response.xpath('//header[@class="main-hd"]/a/span/text()').extract()[0]
        item['date'] = response.xpath('//span[@class="main-meta"]/text()').extract()[0]
        item['url'] = '['+response.xpath('//div[@class="article"]/h1/span/text()').extract()[0]+']('+response.xpath('//meta[@property="og:url"]/@content').extract()[0]+')'
        item['collect'] = int(re.findall(r"\d+\.?\d*",response.xpath('//button[contains(@class,"useful")]/text()').extract()[0])[0])
        item['readview'] = ''.join(response.xpath('//div[@class="review-content clearfix"]//text()').extract())
        
        yield item
```

```py
# -*- coding:utf-8 -*-

import glob, os
import json

import modify as md

# 获得桌上所有 md 文件名并对其进行分割，然后在 for 循环里进行处理
for infile in glob.glob("/Users/Daglas/Desktop/*.json"):
    filename, ext = os.path.splitext(infile)

    # 读取文件，文件名「filename + ".md"」是关键
    with open(filename + ".json") as file_obj:
        dictls = json.load(fp=file_obj)
        # print(dictls[:8])

    views = []
    for reviews in sorted(dictls, key = lambda e:e.__getitem__('collect'), reverse=True):
        views.append(reviews['title'])
        views.append(reviews['author'])
        views.append(reviews['date'])
        views.append(reviews['url'])
        # views.append(reviews['collect'])
        views.append(reviews['readview'])
    
    # print(views[:10])

    # 对文字处理并写入文件
    with open(filename + ".md", 'w') as file_obj:
        for line in views:
            if line != '\n':
                line = line.replace('\n', '\n\n')
                line = md.modify_text(line)
                file_obj.write(line + "\n\n"
```
